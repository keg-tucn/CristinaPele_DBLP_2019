{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesarea datelor\n",
    "1. Transformarea in litere mici\n",
    "2. Verifica lungimea cuvantului (> 3)\n",
    "3. Verifica si elimina caractere de inceput si de sfarsit\n",
    "4. Eliminarea cuvintelor de tip zgomot\n",
    "5. Eliminarea stop_words\n",
    "6. Verificarea partii de vorbire\n",
    "7. Aplica lemmatizare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "from nltk.corpus import wordnet # To get words in dictionary with their parts of speech\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizes word based on it's parts of speech\n",
    "\n",
    "import json\n",
    "import gensim \n",
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminarea cuvintelor de tip zgomot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word(word):\n",
    "    special_chars_start = ['\\'','(', '\"', ':', '“']\n",
    "    special_chars_end = ['\\'',')', '.', ',', '\"', ':', ';' , '”']\n",
    "    if word[0] in special_chars_start:\n",
    "        word = word[1:len(word)]\n",
    "        \n",
    "    if word[len(word) - 1] in special_chars_end:\n",
    "        word = word[0:len(word)-1]\n",
    "    return word.replace(\"-\",\"\")\n",
    "\n",
    "def is_noise(word):    \n",
    "    for c in word:            \n",
    "        if c not in string.ascii_lowercase and c != '-':\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificare partii de vorbire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos( word ):\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "    \n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    return most_common_pos_list[0][0]\n",
    "\n",
    "def is_noun(pos):\n",
    "    if pos == \"n\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_verb(pos):\n",
    "    if pos == \"v\":\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(word, pos):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return (wnl.lemmatize( word, pos ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apelarea preprocesarii pentru abstracte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisierul 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n"
     ]
    }
   ],
   "source": [
    "r = open('../../DBLP/dblp-ref/dblp-final-0.json','r',encoding='utf-8')\n",
    "w = open('dblp-n-0.json','w',encoding='utf-8')\n",
    "i = 0\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    # 1. Transformarea in litere mici\n",
    "    abstract = abstract.lower()\n",
    "    words_list = abstract.split(' ')\n",
    "    new_list = []\n",
    "\n",
    "    for word in words_list: \n",
    "        # 2. Verifica lungimea cuvantului (> 3)\n",
    "        if len(word) > 3:\n",
    "            # 3. Verifica si elimina caractere de inceput, de sfarsit si cratimele \n",
    "            new_word = preprocess_word(word)\n",
    "            \n",
    "            # 4. Eliminarea cuvintelor de tip zgomot\n",
    "            if is_noise(new_word) == False:\n",
    "                \n",
    "                # 5. Eliminarea stop_words\n",
    "                if new_word not in gensim.parsing.preprocessing.STOPWORDS: \n",
    "                    \n",
    "                    # 6. Verificarea partii de vorbire\n",
    "                    pos = get_pos(new_word)\n",
    "                    if is_noun(pos) == True:\n",
    "                    \n",
    "                        # 7. Aplica lemmatizare\n",
    "                        new_word = lemmatize_word(new_word, pos)\n",
    "                        new_list.append(new_word)\n",
    "    \n",
    "    crt_paper['abstract'] = new_list\n",
    "    w.write(json.dumps(crt_paper))\n",
    "    w.write('\\n')\n",
    "    if i % 100000 == 0:\n",
    "        print (i)\n",
    "    i+=1\n",
    "    \n",
    "w.close()\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisierul 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = open('../../DBLP/dblp-ref/dblp-final-1.json','r',encoding='utf-8')\n",
    "w = open('dblp-n-1.json','w',encoding='utf-8')\n",
    "i = 0\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    # 1. Transformarea in litere mici\n",
    "    abstract = abstract.lower()\n",
    "    words_list = abstract.split(' ')\n",
    "    new_list = []\n",
    "\n",
    "    for word in words_list: \n",
    "        # 2. Verifica lungimea cuvantului (> 3)\n",
    "        if len(word) > 3:\n",
    "            # 3. Verifica si elimina caractere de inceput, de sfarsit si cratimele \n",
    "            new_word = preprocess_word(word)\n",
    "            \n",
    "            # 4. Eliminarea cuvintelor de tip zgomot\n",
    "            if is_noise(new_word) == False:\n",
    "                \n",
    "                # 5. Eliminarea stop_words\n",
    "                if new_word not in gensim.parsing.preprocessing.STOPWORDS: \n",
    "                    \n",
    "                    # 6. Verificarea partii de vorbire\n",
    "                    pos = get_pos(new_word)\n",
    "                    if is_noun(pos) == True:\n",
    "                    \n",
    "                        # 7. Aplica lemmatizare\n",
    "                        new_word = lemmatize_word(new_word, pos)\n",
    "                        new_list.append(new_word)\n",
    "    \n",
    "    crt_paper['abstract'] = new_list\n",
    "    w.write(json.dumps(crt_paper))\n",
    "    w.write('\\n')\n",
    "    if i % 100000 == 0:\n",
    "        print (i)\n",
    "    i+=1\n",
    "    \n",
    "w.close()\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisierul 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = open('../../DBLP/dblp-ref/dblp-final-2.json','r',encoding='utf-8')\n",
    "w = open('dblp-n-2.json','w',encoding='utf-8')\n",
    "i = 0\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    # 1. Transformarea in litere mici\n",
    "    abstract = abstract.lower()\n",
    "    words_list = abstract.split(' ')\n",
    "    new_list = []\n",
    "\n",
    "    for word in words_list: \n",
    "        # 2. Verifica lungimea cuvantului (> 3)\n",
    "        if len(word) > 3:\n",
    "            # 3. Verifica si elimina caractere de inceput, de sfarsit si cratimele \n",
    "            new_word = preprocess_word(word)\n",
    "            \n",
    "            # 4. Eliminarea cuvintelor de tip zgomot\n",
    "            if is_noise(new_word) == False:\n",
    "                \n",
    "                # 5. Eliminarea stop_words\n",
    "                if new_word not in gensim.parsing.preprocessing.STOPWORDS: \n",
    "                    \n",
    "                    # 6. Verificarea partii de vorbire\n",
    "                    pos = get_pos(new_word)\n",
    "                    if is_noun(pos) == True:\n",
    "                    \n",
    "                        # 7. Aplica lemmatizare\n",
    "                        new_word = lemmatize_word(new_word, pos)\n",
    "                        new_list.append(new_word)\n",
    "    \n",
    "    crt_paper['abstract'] = new_list\n",
    "    w.write(json.dumps(crt_paper))\n",
    "    w.write('\\n')\n",
    "    if i % 100000 == 0:\n",
    "        print (i)\n",
    "    i+=1\n",
    "    \n",
    "w.close()\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisierul 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = open('../../DBLP/dblp-ref/dblp-final-3.json','r',encoding='utf-8')\n",
    "w = open('dblp-n-3.json','w',encoding='utf-8')\n",
    "i = 0\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    # 1. Transformarea in litere mici\n",
    "    abstract = abstract.lower()\n",
    "    words_list = abstract.split(' ')\n",
    "    new_list = []\n",
    "\n",
    "    for word in words_list: \n",
    "        # 2. Verifica lungimea cuvantului (> 3)\n",
    "        if len(word) > 3:\n",
    "            # 3. Verifica si elimina caractere de inceput, de sfarsit si cratimele \n",
    "            new_word = preprocess_word(word)\n",
    "            \n",
    "            # 4. Eliminarea cuvintelor de tip zgomot\n",
    "            if is_noise(new_word) == False:\n",
    "                \n",
    "                # 5. Eliminarea stop_words\n",
    "                if new_word not in gensim.parsing.preprocessing.STOPWORDS: \n",
    "                    \n",
    "                    # 6. Verificarea partii de vorbire\n",
    "                    pos = get_pos(new_word)\n",
    "                    if is_noun(pos) == True:\n",
    "                    \n",
    "                        # 7. Aplica lemmatizare\n",
    "                        new_word = lemmatize_word(new_word, pos)\n",
    "                        new_list.append(new_word)\n",
    "    \n",
    "    crt_paper['abstract'] = new_list\n",
    "    w.write(json.dumps(crt_paper))\n",
    "    w.write('\\n')\n",
    "    if i % 100000 == 0:\n",
    "        print (i)\n",
    "    i+=1\n",
    "    \n",
    "w.close()\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creare lista de cuvinte unice din fisierul 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n"
     ]
    }
   ],
   "source": [
    "word_list0 = []\n",
    "r = open('dblp-n-0.json','r',encoding='utf-8')\n",
    "i = 0\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    \n",
    "    word_list0.extend(abstract)\n",
    "    if i % 50000 == 0:\n",
    "        print (i)\n",
    "    i+=1\n",
    "\n",
    "r.close()\n",
    "\n",
    "## Eliminare duplicate\n",
    "word_list0 = list(dict.fromkeys(word_list0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scriere lista 0 in fisier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list-n0.json','wb') as w:\n",
    "    pickle.dump(word_list0, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429391\n"
     ]
    }
   ],
   "source": [
    "print(len(word_list0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creare lista de cuvinte unice din fisierul 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list1 = []\n",
    "r = open('dblp-n-1.json','r',encoding='utf-8')\n",
    "i = 0\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    \n",
    "    word_list1.extend(abstract)\n",
    "    if i % 50000 == 0:\n",
    "        print (i)\n",
    "    i+=1\n",
    "\n",
    "r.close()\n",
    "\n",
    "## Eliminare duplicate\n",
    "word_list1 = list(dict.fromkeys(word_list1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scriere lista 1 in fisier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list-n1.json','wb') as w:\n",
    "    pickle.dump(word_list1, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_list1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creare lista de cuvinte unice din fisierul 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list2 = []\n",
    "r = open('dblp-n-2.json','r',encoding='utf-8')\n",
    "i = 0\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    \n",
    "    word_list2.extend(abstract)\n",
    "    if i % 50000 == 0:\n",
    "        print (i)\n",
    "    i+=1\n",
    "\n",
    "r.close()\n",
    "\n",
    "## Eliminare duplicate\n",
    "word_list2 = list(dict.fromkeys(word_list2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scriere lista 2 in fisier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list-n2.json','wb') as w:\n",
    "    pickle.dump(word_list2, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creare lista de cuvinte unice din fisierul 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list3 = []\n",
    "r = open('dblp-n-3.json','r',encoding='utf-8')\n",
    "i = 0\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    \n",
    "    word_list3.extend(abstract)\n",
    "    if i % 50000 == 0:\n",
    "        print (i)\n",
    "    i+=1\n",
    "\n",
    "r.close()\n",
    "\n",
    "## Eliminare duplicate\n",
    "word_list3 = list(dict.fromkeys(word_list3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scriere lista 3 in fisier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list-n3.json','wb') as w:\n",
    "    pickle.dump(word_list3, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinarea listelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citire liste\n",
    "wlist0 = []\n",
    "wlist1 = []\n",
    "wlist2 = []\n",
    "wlist3 = []\n",
    "with open('list-n0.json','rb') as r:\n",
    "    wlist0 = pickle.load(r)\n",
    "with open('list-n1.json','rb') as r:\n",
    "    wlist1 = pickle.load(r)\n",
    "with open('list-n2.json','rb') as r:\n",
    "    wlist2 = pickle.load(r)\n",
    "with open('list-n3.json','rb') as r:\n",
    "    wlist3 = pickle.load(r)\n",
    "\n",
    "# Combinarea liste 0 cu 1\n",
    "wlist0[len(wlist0):] = wlist1 \n",
    "\n",
    "# Combinarea liste 2 cu 3\n",
    "wlist2[len(wlist2):] = wlist3 \n",
    "\n",
    "# Combinarea listei 01 cu lista 23\n",
    "wlist0[len(wlist0):] = wlist2\n",
    "\n",
    "# Eliminare duplicate\n",
    "list_final = []\n",
    "list_final = list(dict.fromkeys(wlist0))\n",
    "\n",
    "#Scrie in fisier lista finala\n",
    "with open('list-n-final.json','wb') as w:\n",
    "    pickle.dump(list_final, w)\n",
    "    \n",
    "NUM_OF_WORDS = len(list_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creare dictionar de cuvinte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citire lista de cuvinte\n",
    "word_list = []\n",
    "with open('list-n-final.json','rb') as r:\n",
    "    word_list = pickle.load(r)\n",
    "    \n",
    "lista_index = []\n",
    "for i in range(NUM_OF_WORDS):\n",
    "    lista_index.append(i)\n",
    "dict_words = dict(zip(word_list,lista_index))\n",
    "\n",
    "# Scrie in fisier dictionarul\n",
    "with open('dict-n.json','wb') as w:\n",
    "    pickle.dump(dict_words, w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
