{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesarea datelor\n",
    "1. Transformarea in litere mici\n",
    "2. Verifica lungimea cuvantului (> 3)\n",
    "3. Verifica si elimina caractere de inceput si de sfarsit\n",
    "4. Eliminarea cuvintelor de tip zgomot\n",
    "5. Eliminarea stop_words\n",
    "6. Verificarea partii de vorbire\n",
    "7. Aplica lemmatizare\n",
    "8. Eliminarea cuvintelor ce au numarul de aparitii sub si peste treshlod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "from nltk.corpus import wordnet # To get words in dictionary with their parts of speech\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizes word based on it's parts of speech\n",
    "\n",
    "import json\n",
    "import gensim \n",
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Eliminarea cuvintelor de tip zgomot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word(word):\n",
    "    special_chars_start = ['\\'','(', '\"', ':', '“']\n",
    "    special_chars_end = ['\\'',')', '.', ',', '\"', ':', ';' , '”']\n",
    "    if word[0] in special_chars_start:\n",
    "        word = word[1:len(word)]\n",
    "        \n",
    "    if word[len(word) - 1] in special_chars_end:\n",
    "        word = word[0:len(word)-1]\n",
    "    return word.replace(\"-\",\"\")\n",
    "\n",
    "def is_noise(word):    \n",
    "    for c in word:            \n",
    "        if c not in string.ascii_lowercase and c != '-':\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verificare partii de vorbire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos( word ):\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "    \n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    return most_common_pos_list[0][0]\n",
    "\n",
    "def is_noun(pos):\n",
    "    if pos == \"n\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_verb(pos):\n",
    "    if pos == \"v\":\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lemmatizare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(word, pos):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return (wnl.lemmatize( word, pos ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apelarea preprocesarii pentru abstracte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisierul 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n"
     ]
    }
   ],
   "source": [
    "r = open('../../DBLP/dblp-ref/dblp-final-0.json','r',encoding='utf-8')\n",
    "w = open('dblp-vn-0.json','w',encoding='utf-8')\n",
    "i = 0\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    # 1. Transformarea in litere mici\n",
    "    abstract = abstract.lower()\n",
    "    words_list = abstract.split(' ')\n",
    "    new_list = []\n",
    "\n",
    "    for word in words_list: \n",
    "        # 2. Verifica lungimea cuvantului (> 3)\n",
    "        if len(word) > 3:\n",
    "            # 3. Verifica si elimina caractere de inceput, de sfarsit si cratimele \n",
    "            new_word = preprocess_word(word)\n",
    "            \n",
    "            # 4. Eliminarea cuvintelor de tip zgomot\n",
    "            if is_noise(new_word) == False:\n",
    "                \n",
    "                # 5. Eliminarea stop_words\n",
    "                if new_word not in gensim.parsing.preprocessing.STOPWORDS: \n",
    "                    \n",
    "                    # 6. Verificarea partii de vorbire\n",
    "                    pos = get_pos(new_word)\n",
    "                    if is_noun(pos) == True or is_verb(pos) == True:\n",
    "                    \n",
    "                        # 7. Aplica lemmatizare\n",
    "                        new_word = lemmatize_word(new_word, pos)\n",
    "                        new_list.append(new_word)\n",
    "    \n",
    "    crt_paper['abstract'] = new_list\n",
    "    w.write(json.dumps(crt_paper))\n",
    "    w.write('\\n')\n",
    "    if i % 100000 == 0:\n",
    "        print (i)\n",
    "    i+=1\n",
    "    \n",
    "w.close()\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisierul 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n"
     ]
    }
   ],
   "source": [
    "r = open('../../DBLP/dblp-ref/dblp-final-1.json','r',encoding='utf-8')\n",
    "w = open('dblp-vn-1.json','w',encoding='utf-8')\n",
    "i = 0\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    # 1. Transformarea in litere mici\n",
    "    abstract = abstract.lower()\n",
    "    words_list = abstract.split(' ')\n",
    "    new_list = []\n",
    "\n",
    "    for word in words_list: \n",
    "        # 2. Verifica lungimea cuvantului (> 3)\n",
    "        if len(word) > 3:\n",
    "            # 3. Verifica si elimina caractere de inceput, de sfarsit si cratimele \n",
    "            new_word = preprocess_word(word)\n",
    "            \n",
    "            # 4. Eliminarea cuvintelor de tip zgomot\n",
    "            if is_noise(new_word) == False:\n",
    "                \n",
    "                # 5. Eliminarea stop_words\n",
    "                if new_word not in gensim.parsing.preprocessing.STOPWORDS: \n",
    "                    \n",
    "                    # 6. Verificarea partii de vorbire\n",
    "                    pos = get_pos(new_word)\n",
    "                    if is_noun(pos) == True or is_verb(pos) == True:\n",
    "                    \n",
    "                        # 7. Aplica lemmatizare\n",
    "                        new_word = lemmatize_word(new_word, pos)\n",
    "                        new_list.append(new_word)\n",
    "    \n",
    "    crt_paper['abstract'] = new_list\n",
    "    w.write(json.dumps(crt_paper))\n",
    "    w.write('\\n')\n",
    "    if i % 100000 == 0:\n",
    "        print (i)\n",
    "    i+=1\n",
    "    \n",
    "w.close()\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisierul 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n"
     ]
    }
   ],
   "source": [
    "r = open('../../DBLP/dblp-ref/dblp-final-2.json','r',encoding='utf-8')\n",
    "w = open('dblp-vn-2.json','w',encoding='utf-8')\n",
    "i = 0\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    # 1. Transformarea in litere mici\n",
    "    abstract = abstract.lower()\n",
    "    words_list = abstract.split(' ')\n",
    "    new_list = []\n",
    "\n",
    "    for word in words_list: \n",
    "        # 2. Verifica lungimea cuvantului (> 3)\n",
    "        if len(word) > 3:\n",
    "            # 3. Verifica si elimina caractere de inceput, de sfarsit si cratimele \n",
    "            new_word = preprocess_word(word)\n",
    "            \n",
    "            # 4. Eliminarea cuvintelor de tip zgomot\n",
    "            if is_noise(new_word) == False:\n",
    "                \n",
    "                # 5. Eliminarea stop_words\n",
    "                if new_word not in gensim.parsing.preprocessing.STOPWORDS: \n",
    "                    \n",
    "                    # 6. Verificarea partii de vorbire\n",
    "                    pos = get_pos(new_word)\n",
    "                    if is_noun(pos) == True or is_verb(pos) == True:\n",
    "                    \n",
    "                        # 7. Aplica lemmatizare\n",
    "                        new_word = lemmatize_word(new_word, pos)\n",
    "                        new_list.append(new_word)\n",
    "    \n",
    "    crt_paper['abstract'] = new_list\n",
    "    w.write(json.dumps(crt_paper))\n",
    "    w.write('\\n')\n",
    "    if i % 100000 == 0:\n",
    "        print (i)\n",
    "    i+=1\n",
    "    \n",
    "w.close()\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisierul 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "r = open('../../DBLP/dblp-ref/dblp-final-3.json','r',encoding='utf-8')\n",
    "w = open('dblp-vn-3.json','w',encoding='utf-8')\n",
    "i = 0\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    # 1. Transformarea in litere mici\n",
    "    abstract = abstract.lower()\n",
    "    words_list = abstract.split(' ')\n",
    "    new_list = []\n",
    "\n",
    "    for word in words_list: \n",
    "        # 2. Verifica lungimea cuvantului (> 3)\n",
    "        if len(word) > 3:\n",
    "            # 3. Verifica si elimina caractere de inceput, de sfarsit si cratimele \n",
    "            new_word = preprocess_word(word)\n",
    "            \n",
    "            # 4. Eliminarea cuvintelor de tip zgomot\n",
    "            if is_noise(new_word) == False:\n",
    "                \n",
    "                # 5. Eliminarea stop_words\n",
    "                if new_word not in gensim.parsing.preprocessing.STOPWORDS: \n",
    "                    \n",
    "                    # 6. Verificarea partii de vorbire\n",
    "                    pos = get_pos(new_word)\n",
    "                    if is_noun(pos) == True or is_verb(pos) == True:\n",
    "                    \n",
    "                        # 7. Aplica lemmatizare\n",
    "                        new_word = lemmatize_word(new_word, pos)\n",
    "                        new_list.append(new_word)\n",
    "    \n",
    "    crt_paper['abstract'] = new_list\n",
    "    w.write(json.dumps(crt_paper))\n",
    "    w.write('\\n')\n",
    "    if i % 100000 == 0:\n",
    "        print (i)\n",
    "    i+=1\n",
    "    \n",
    "w.close()\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creare lista de cuvinte unice din fisierul 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list0 = []\n",
    "r = open('dblp-vn-0.json','r',encoding='utf-8')\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    word_list0.extend(abstract)\n",
    "    \n",
    "r.close()\n",
    "\n",
    "## Eliminare duplicate\n",
    "word_list0 = list(dict.fromkeys(word_list0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scriere lista 0 in fisier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list-vn0.json','wb') as w:\n",
    "    pickle.dump(word_list0, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433159\n"
     ]
    }
   ],
   "source": [
    "print(len(word_list0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creare lista de cuvinte unice din fisierul 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list1 = []\n",
    "r = open('dblp-vn-1.json','r',encoding='utf-8')\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    word_list1.extend(abstract)\n",
    "\n",
    "r.close()\n",
    "\n",
    "## Eliminare duplicate\n",
    "word_list1 = list(dict.fromkeys(word_list1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scriere lista 1 in fisier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list-vn1.json','wb') as w:\n",
    "    pickle.dump(word_list1, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518426\n"
     ]
    }
   ],
   "source": [
    "print(len(word_list1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creare lista de cuvinte unice din fisierul 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list2 = []\n",
    "r = open('dblp-vn-2.json','r',encoding='utf-8')\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    word_list2.extend(abstract)\n",
    "\n",
    "r.close()\n",
    "\n",
    "## Eliminare duplicate\n",
    "word_list2 = list(dict.fromkeys(word_list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scriere lista 2 in fisier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list-vn2.json','wb') as w:\n",
    "    pickle.dump(word_list2, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487270\n"
     ]
    }
   ],
   "source": [
    "print(len(word_list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creare lista de cuvinte unice din fisierul 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list3 = []\n",
    "r = open('dblp-vn-3.json','r',encoding='utf-8')\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    word_list3.extend(abstract)\n",
    "\n",
    "r.close()\n",
    "\n",
    "## Eliminare duplicate\n",
    "word_list3 = list(dict.fromkeys(word_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scriere lista 3 in fisier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list-vn3.json','wb') as w:\n",
    "    pickle.dump(word_list3, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64073\n"
     ]
    }
   ],
   "source": [
    "print(len(word_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinarea listelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citire liste\n",
    "wlist0 = []\n",
    "wlist1 = []\n",
    "wlist2 = []\n",
    "wlist3 = []\n",
    "with open('list-vn0.json','rb') as r:\n",
    "    wlist0 = pickle.load(r)\n",
    "with open('list-vn1.json','rb') as r:\n",
    "    wlist1 = pickle.load(r)\n",
    "with open('list-vn2.json','rb') as r:\n",
    "    wlist2 = pickle.load(r)\n",
    "with open('list-vn3.json','rb') as r:\n",
    "    wlist3 = pickle.load(r)\n",
    "\n",
    "# Combinarea liste 0 cu 1\n",
    "wlist0[len(wlist0):] = wlist1 \n",
    "\n",
    "# Combinarea liste 2 cu 3\n",
    "wlist2[len(wlist2):] = wlist3 \n",
    "\n",
    "# Combinarea listei 01 cu lista 23\n",
    "wlist0[len(wlist0):] = wlist2\n",
    "\n",
    "# Eliminare duplicate\n",
    "list_final = []\n",
    "list_final = list(dict.fromkeys(wlist0))\n",
    "\n",
    "#Scrie in fisier lista finala\n",
    "with open('list-vn-final.json','wb') as w:\n",
    "    pickle.dump(list_final, w)\n",
    "    \n",
    "NUM_OF_WORDS = len(list_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988791\n"
     ]
    }
   ],
   "source": [
    "print(NUM_OF_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creare dictionar de cuvinte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citire lista de cuvinte\n",
    "word_list = []\n",
    "with open('list-vn-final.json','rb') as r:\n",
    "    word_list = pickle.load(r)\n",
    "    \n",
    "lista_index = []\n",
    "for i in range(NUM_OF_WORDS):\n",
    "    lista_index.append(i)\n",
    "dict_words = dict(zip(word_list,lista_index))\n",
    "\n",
    "# Scrie in fisier dictionarul\n",
    "with open('dict-vn.json','wb') as w:\n",
    "    pickle.dump(dict_words, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creare lista de aparitii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_aparitii = []\n",
    "for i in range(NUM_OF_WORDS):\n",
    "    lista_aparitii.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parcurge fisierul 0 si incrementeaza lista de aparitii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = open('dblp-vn-0.json','r',encoding='utf-8')\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    \n",
    "    for word in abstract:\n",
    "        lista_aparitii[dict_words[word]] = lista_aparitii[dict_words[word]] + 1 \n",
    "\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parcurge fisierul 1 si incrementeaza lista de aparitii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = open('dblp-vn-1.json','r',encoding='utf-8')\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    \n",
    "    for word in abstract:\n",
    "        lista_aparitii[dict_words[word]] = lista_aparitii[dict_words[word]] + 1 \n",
    "\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parcurge fisierul 2 si incrementeaza lista de aparitii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = open('dblp-vn-2.json','r',encoding='utf-8')\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    \n",
    "    for word in abstract:\n",
    "        lista_aparitii[dict_words[word]] = lista_aparitii[dict_words[word]] + 1 \n",
    "\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parcurge fisierul 3 si incrementeaza lista de aparitii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = open('dblp-vn-3.json','r',encoding='utf-8')\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "    \n",
    "    for word in abstract:\n",
    "        lista_aparitii[dict_words[word]] = lista_aparitii[dict_words[word]] + 1 \n",
    "\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrie in fisier lista de aparitii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lista-aparitii-vn.json','wb') as w:\n",
    "    pickle.dump(lista_aparitii, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Eliminarea cuvintelor ce au numarul de aparitii sub si peste treshlod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 3\n",
    "upper_bound = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citire lista de aparitii\n",
    "lista_aparitii = []\n",
    "with open('lista-aparitii-vn.json','rb') as r:\n",
    "    lista_aparitii = pickle.load(r)\n",
    "    \n",
    "# Citire dictionar\n",
    "dict_words = []\n",
    "with open('dict-vn.json','rb') as r:\n",
    "    dict_words = pickle.load(r)\n",
    "    \n",
    "# Eliminare cuvintelor din fisierul 0\n",
    "r = open('dblp-vn-0.json','r',encoding='utf-8')\n",
    "w = open('dblp-reduced-vn-0.json','w',encoding='utf-8')\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "\n",
    "    new_abstract = []\n",
    "    for word in abstract:\n",
    "        if (lista_aparitii[dict_words[word]] >= lower_bound and lista_aparitii[dict_words[word]] <= upper_bound):\n",
    "            new_abstract.append(word)\n",
    "\n",
    "    crt_paper['abstract'] = new_abstract\n",
    "    w.write(json.dumps(crt_paper))\n",
    "    w.write('\\n')\n",
    "    \n",
    "w.close()\n",
    "r.close()\n",
    "\n",
    "# Eliminare cuvintelor din fisierul 1\n",
    "r = open('dblp-vn-1.json','r',encoding='utf-8')\n",
    "w = open('dblp-reduced-vn-1.json','w',encoding='utf-8')\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "\n",
    "    new_abstract = []\n",
    "    for word in abstract:\n",
    "        if (lista_aparitii[dict_words[word]] >= lower_bound and lista_aparitii[dict_words[word]] <= upper_bound):\n",
    "            new_abstract.append(word)\n",
    "\n",
    "    crt_paper['abstract'] = new_abstract\n",
    "    w.write(json.dumps(crt_paper))\n",
    "    w.write('\\n')\n",
    "    \n",
    "w.close()\n",
    "r.close()\n",
    "\n",
    "# Eliminare cuvintelor din fisierul 2\n",
    "r = open('dblp-vn-2.json','r',encoding='utf-8')\n",
    "w = open('dblp-reduced-vn-2.json','w',encoding='utf-8')\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "\n",
    "    new_abstract = []\n",
    "    for word in abstract:\n",
    "        if (lista_aparitii[dict_words[word]] >= lower_bound and lista_aparitii[dict_words[word]] <= upper_bound):\n",
    "            new_abstract.append(word)\n",
    "\n",
    "    crt_paper['abstract'] = new_abstract\n",
    "    w.write(json.dumps(crt_paper))\n",
    "    w.write('\\n')\n",
    "    \n",
    "w.close()\n",
    "r.close()\n",
    "\n",
    "\n",
    "# Eliminare cuvintelor din fisierul 3\n",
    "r = open('dblp-vn-3.json','r',encoding='utf-8')\n",
    "w = open('dblp-reduced-vn-3.json','w',encoding='utf-8')\n",
    "\n",
    "for line in r:\n",
    "    crt_paper = json.loads(line)\n",
    "    abstract = crt_paper['abstract']\n",
    "\n",
    "    new_abstract = []\n",
    "    for word in abstract:\n",
    "        if (lista_aparitii[dict_words[word]] >= lower_bound and lista_aparitii[dict_words[word]] <= upper_bound):\n",
    "            new_abstract.append(word)\n",
    "\n",
    "    crt_paper['abstract'] = new_abstract\n",
    "    w.write(json.dumps(crt_paper))\n",
    "    w.write('\\n')\n",
    "    \n",
    "w.close()\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calcularea listei reduse \n",
    "\n",
    "# Citirea listei de aparitii\n",
    "lista_aparitii = []\n",
    "with open('lista-aparitii-vn.json','rb') as r:\n",
    "    lista_aparitii = pickle.load(r)\n",
    "\n",
    "# Citirea listei de cuvinte\n",
    "lista_cuvinte = []\n",
    "with open('list-vn-final.json','rb') as r:\n",
    "    lista_cuvinte = pickle.load(r)\n",
    "    \n",
    "# Citirea dictionarului de cuvinte\n",
    "dict_words = []\n",
    "with open('dict-vn.json','rb') as r:\n",
    "    dict_words = pickle.load(r)\n",
    "    \n",
    "# Calcularea noii liste\n",
    "new_list = []\n",
    "for word in lista_cuvinte:\n",
    "    if (lista_aparitii[dict_words[word]] >= lower_bound and lista_aparitii[dict_words[word]] <= upper_bound):\n",
    "        new_list.append(word)\n",
    "        \n",
    "# Scrie lista redusa in fisier\n",
    "with open('list-reduced-vn.json','wb') as w:\n",
    "    pickle.dump(new_list, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343588\n"
     ]
    }
   ],
   "source": [
    "print(len(new_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afisare statistici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1358651, 'model'], [1315297, 'propose'], [1240719, 'paper'], [1135446, 'method'], [1119199, 'algorithm'], [1079469, 'result'], [1025689, 'data'], [912367, 'network'], [902944, 'problem'], [880822, 'base'], [811460, 'present'], [808309, 'approach'], [692483, 'performance'], [617371, 'time'], [607517, 'process'], [600900, 'design'], [583243, 'information'], [572042, 'provide'], [568284, 'system'], [554874, 'study'], [547454, 'application'], [503180, 'image'], [485642, 'technique'], [472185, 'analysis'], [456653, 'user'], [441572, 'number'], [414262, 'control'], [388047, 'scheme'], [380112, 'feature'], [369423, 'function'], [356049, 'develop'], [352622, 'work'], [342529, 'test'], [340214, 'learn'], [337686, 'simulation'], [333818, 'order'], [332988, 'solution'], [326709, 'improve'], [323714, 'compare'], [320294, 'show'], [317988, 'structure'], [317644, 'consider'], [316016, 'service'], [302706, 'case'], [289518, 'apply'], [286910, 'give'], [284610, 'achieve'], [284138, 'obtain'], [280465, 'support'], [275734, 'framework']]\n",
      "[[1, 'colourants'], [1, 'memoryisolated'], [1, 'xax'], [1, 'languagewide'], [1, 'piqasso'], [1, 'motautomate'], [1, 'expositionof'], [1, 'iscritically'], [1, 'dualof'], [1, 'hypernegation'], [1, 'complementrelative'], [1, 'thissecond'], [1, 'thesyllogistic'], [1, 'senseclustered'], [1, 'economicallybased'], [1, 'patchingrelated'], [1, 'loyment'], [1, 'specificati'], [1, 'ciocan'], [1, 'simililarity'], [1, 'iaarf'], [1, 'coaxal'], [1, 'commensu'], [1, 'couceiro'], [1, 'worldfeatures'], [1, 'mousedrag'], [1, 'vwhich'], [1, 'handtree'], [1, 'userly'], [1, 'subintentional'], [1, 'sadjad'], [1, 'carriersensingbased'], [1, 'dustgrainsized'], [1, 'yileding'], [1, 'sizekn'], [1, 'programschema'], [1, 'nonindefinites'], [1, 'clausedensity'], [1, 'longrising'], [1, 'starfleet'], [1, 'recompound'], [1, 'semanticrelative'], [1, 'kmix'], [1, 'lobnef'], [1, 'statisfiability'], [1, 'votingscheme'], [1, 'sharerec'], [1, 'clony'], [1, 'sctios'], [1, 'networkcritical']]\n",
      "colourants\n",
      ",\n",
      "memoryisolated\n",
      ",\n",
      "xax\n",
      ",\n",
      "languagewide\n",
      ",\n",
      "piqasso\n",
      ",\n",
      "motautomate\n",
      ",\n",
      "expositionof\n",
      ",\n",
      "iscritically\n",
      ",\n",
      "dualof\n",
      ",\n",
      "hypernegation\n",
      ",\n",
      "complementrelative\n",
      ",\n",
      "thissecond\n",
      ",\n",
      "thesyllogistic\n",
      ",\n",
      "senseclustered\n",
      ",\n",
      "economicallybased\n",
      ",\n",
      "patchingrelated\n",
      ",\n",
      "loyment\n",
      ",\n",
      "specificati\n",
      ",\n",
      "ciocan\n",
      ",\n",
      "simililarity\n",
      ",\n",
      "iaarf\n",
      ",\n",
      "coaxal\n",
      ",\n",
      "commensu\n",
      ",\n",
      "couceiro\n",
      ",\n",
      "worldfeatures\n",
      ",\n",
      "mousedrag\n",
      ",\n",
      "vwhich\n",
      ",\n",
      "handtree\n",
      ",\n",
      "userly\n",
      ",\n",
      "subintentional\n",
      ",\n",
      "sadjad\n",
      ",\n",
      "carriersensingbased\n",
      ",\n",
      "dustgrainsized\n",
      ",\n",
      "yileding\n",
      ",\n",
      "sizekn\n",
      ",\n",
      "programschema\n",
      ",\n",
      "nonindefinites\n",
      ",\n",
      "clausedensity\n",
      ",\n",
      "longrising\n",
      ",\n",
      "starfleet\n",
      ",\n",
      "recompound\n",
      ",\n",
      "semanticrelative\n",
      ",\n",
      "kmix\n",
      ",\n",
      "lobnef\n",
      ",\n",
      "statisfiability\n",
      ",\n",
      "votingscheme\n",
      ",\n",
      "sharerec\n",
      ",\n",
      "clony\n",
      ",\n",
      "sctios\n",
      ",\n",
      "networkcritical\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "## Calcularea listei reduse \n",
    "\n",
    "# Citirea listei de aparitii\n",
    "lista_aparitii = []\n",
    "with open('lista-aparitii-vn.json','rb') as r:\n",
    "    lista_aparitii = pickle.load(r)\n",
    "\n",
    "# Citirea listei de cuvinte\n",
    "lista_cuvinte = []\n",
    "with open('list-vn-final.json','rb') as r:\n",
    "    lista_cuvinte = pickle.load(r)\n",
    "    \n",
    "# Citirea dictionarului de cuvinte\n",
    "dict_words = []\n",
    "with open('dict-vn.json','rb') as r:\n",
    "    dict_words = pickle.load(r)\n",
    "    \n",
    "# Inserarea ordonata intr-o noua lista de liste\n",
    "listl = []\n",
    "i = 0\n",
    "for word in lista_cuvinte:   \n",
    "    listl.append([lista_aparitii[dict_words[word]],word])\n",
    "    \n",
    "from operator import itemgetter\n",
    "sl = sorted(listl, key=itemgetter(0))\n",
    "\n",
    "most_frecv = sl[len(sl)-50:]\n",
    "print(most_frecv[::-1])\n",
    "\n",
    "least_frecv = sl[0:50]\n",
    "print(least_frecv)\n",
    "for elem in least_frecv:\n",
    "    print(elem[1])\n",
    "    print(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
