{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCES DATA\n",
    "### 1. Elimina articolele care nu au: id, title, autors, venue, year, keywords, fos, references, n_citation, lang (!=en), abstract sau indexed_abstract\n",
    "### 2. Pune cate 50000 de articole intr-un fisier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.corpus import stopwords # pentru eliminare stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizes word based on it's parts of speech\n",
    "import gensim\n",
    "from nltk.corpus import wordnet # To get words in dictionary with their parts of speech\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elimina_articole_incomplete(fileIn, fileOut):\n",
    "    fileOut = 'completeData/' + fileOut\n",
    "    wrongPapers = 0\n",
    "    okPapers = 0\n",
    "    i = 0\n",
    "    r = open(fileIn,'r',encoding='utf-8')\n",
    "    w = open(fileOut,'w',encoding='utf-8')\n",
    "    for line in r:\n",
    "        if i % 1000000 == 0:\n",
    "            print(i, wrongPapers, okPapers)\n",
    "        i += 1\n",
    "        crt_paper = json.loads(line)\n",
    "        try:\n",
    "            lang_cp = crt_paper[\"venue\"] \n",
    "        except:\n",
    "            wrongPapers += 1\n",
    "            continue\n",
    "        \n",
    "        okPapers += 1\n",
    "        w.write(json.dumps(crt_paper))\n",
    "        w.write('\\n')\n",
    "            \n",
    "    print(fileIn, wrongPapers, okPapers)\n",
    "    w.close()\n",
    "    r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n",
      "1000000 2821 997179\n",
      "2000000 3520 1996480\n",
      "3000000 7575 2992425\n",
      "./completeData/dblp_v11_4.json 12693 3257719\n"
     ]
    }
   ],
   "source": [
    "elimina_articole_incomplete('./completeData/dblp_v11_4.json','dblp_v11_5.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_articles():\n",
    "    r = open('completeData/dblp_v11_5.json','r',encoding='utf-8')\n",
    "    nr = 1\n",
    "    current_paper = 1\n",
    "    w = open('splited_data/file0.txt','w',encoding='utf-8')\n",
    "    \n",
    "    for line in r:\n",
    "        if current_paper % 100000 == 0:\n",
    "            w.close()\n",
    "            currentFile = 'splited_data/file' + str(nr) + '.txt'\n",
    "            nr += 1\n",
    "            w =  open(currentFile,'w',encoding='utf-8')\n",
    "        current_paper += 1\n",
    "        \n",
    "        crt_paper = json.loads(line)\n",
    "        w.write(json.dumps(crt_paper))\n",
    "        w.write('\\n')\n",
    "            \n",
    "    w.close()\n",
    "    r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elimina_punctuatia(word):\n",
    "    new_word = \"\"\n",
    "    for ch in word:\n",
    "        if ch not in string.punctuation:\n",
    "            new_word += ch\n",
    "    new_word = re.sub(r'\\d+', '', new_word)\n",
    "    return new_word\n",
    "\n",
    "def get_pos( word ):\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "    \n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    return most_common_pos_list[0][0]\n",
    "\n",
    "def is_noun(pos):\n",
    "    if pos == \"n\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_verb(pos):\n",
    "    if pos == \"v\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def lemmatize_word(word, pos):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return (wnl.lemmatize( word, pos ))\n",
    "\n",
    "def preprocess_indexabs(fileIn,fileOut):\n",
    "    w = open(fileOut,'w',encoding='utf-8')\n",
    "    r = open(fileIn,'r',encoding='utf-8')\n",
    "    for line in r:\n",
    "        crt_paper = json.loads(line)\n",
    "        iabstr = crt_paper[\"indexed_abstract\"][\"InvertedIndex\"] \n",
    "        new_iabstr = {}\n",
    "        for elem in iabstr:\n",
    "            new_elem = elem.lower()\n",
    "            new_elem = elimina_punctuatia(new_elem)\n",
    "            if new_elem in gensim.parsing.preprocessing.STOPWORDS:\n",
    "                continue\n",
    "            if len(new_elem) < 3:\n",
    "                continue\n",
    "            if not is_verb(get_pos(new_elem)):\n",
    "                continue\n",
    "            new_elem = lemmatize_word(new_elem, \"v\")\n",
    "            nr_aparitii = len(iabstr[elem])\n",
    "            try :\n",
    "                new_iabstr[new_elem] += nr_aparitii\n",
    "            except:\n",
    "                new_iabstr[new_elem] = nr_aparitii\n",
    "\n",
    "\n",
    "        crt_paper[\"indexed_abstract\"]  = new_iabstr \n",
    "        w.write(json.dumps(crt_paper))\n",
    "        w.write('\\n')\n",
    "    r.close()\n",
    "        \n",
    "    w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splited_data/file0.txt preprocessedIA/file0.txt\n",
      "splited_data/file1.txt preprocessedIA/file1.txt\n",
      "splited_data/file2.txt preprocessedIA/file2.txt\n",
      "splited_data/file3.txt preprocessedIA/file3.txt\n",
      "splited_data/file4.txt preprocessedIA/file4.txt\n",
      "splited_data/file5.txt preprocessedIA/file5.txt\n",
      "splited_data/file6.txt preprocessedIA/file6.txt\n",
      "splited_data/file7.txt preprocessedIA/file7.txt\n",
      "splited_data/file8.txt preprocessedIA/file8.txt\n",
      "splited_data/file9.txt preprocessedIA/file9.txt\n",
      "splited_data/file10.txt preprocessedIA/file10.txt\n",
      "splited_data/file11.txt preprocessedIA/file11.txt\n",
      "splited_data/file12.txt preprocessedIA/file12.txt\n",
      "splited_data/file13.txt preprocessedIA/file13.txt\n",
      "splited_data/file14.txt preprocessedIA/file14.txt\n",
      "splited_data/file15.txt preprocessedIA/file15.txt\n",
      "splited_data/file16.txt preprocessedIA/file16.txt\n",
      "splited_data/file17.txt preprocessedIA/file17.txt\n",
      "splited_data/file18.txt preprocessedIA/file18.txt\n",
      "splited_data/file19.txt preprocessedIA/file19.txt\n",
      "splited_data/file20.txt preprocessedIA/file20.txt\n",
      "splited_data/file21.txt preprocessedIA/file21.txt\n",
      "splited_data/file22.txt preprocessedIA/file22.txt\n",
      "splited_data/file23.txt preprocessedIA/file23.txt\n",
      "splited_data/file24.txt preprocessedIA/file24.txt\n",
      "splited_data/file25.txt preprocessedIA/file25.txt\n",
      "splited_data/file26.txt preprocessedIA/file26.txt\n",
      "splited_data/file27.txt preprocessedIA/file27.txt\n",
      "splited_data/file28.txt preprocessedIA/file28.txt\n",
      "splited_data/file29.txt preprocessedIA/file29.txt\n",
      "splited_data/file30.txt preprocessedIA/file30.txt\n",
      "splited_data/file31.txt preprocessedIA/file31.txt\n",
      "splited_data/file32.txt preprocessedIA/file32.txt\n"
     ]
    }
   ],
   "source": [
    "fileIn = 'splited_data/file'\n",
    "fileOut = 'preprocessedIA/file'\n",
    "for i in range(0,33):\n",
    "    fileIn1 = fileIn + str(i) + '.txt'\n",
    "    fileOut1 = fileOut + str(i) + '.txt'\n",
    "    preprocess_indexabs(fileIn1,fileOut1 )\n",
    "    print(fileIn1,fileOut1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grupare cuvinte abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grupare_abstracte(word_dict,fileIn,fileOut):\n",
    "    #w = open(fileOut,'w',encoding='utf-8')\n",
    "    r = open(fileIn,'r',encoding='utf-8')\n",
    "    \n",
    "    for line in r:\n",
    "        crt_paper = json.loads(line)\n",
    "        iabstr = crt_paper[\"indexed_abstract\"]\n",
    "        \n",
    "        for elem in iabstr:\n",
    "            try:\n",
    "                word_dict[elem] += iabstr[elem]\n",
    "            except:\n",
    "                word_dict[elem] = iabstr[elem]\n",
    "\n",
    "    #w.write(json.dumps(word_dict))\n",
    "    print(len(word_dict))\n",
    "    #w.close()\n",
    "    r.close()\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4039\n",
      "preprocessedIA/file0.txt IAgroups/dict0.txt\n",
      "4519\n",
      "preprocessedIA/file1.txt IAgroups/dict1.txt\n",
      "4791\n",
      "preprocessedIA/file2.txt IAgroups/dict2.txt\n",
      "4996\n",
      "preprocessedIA/file3.txt IAgroups/dict3.txt\n",
      "5152\n",
      "preprocessedIA/file4.txt IAgroups/dict4.txt\n",
      "5307\n",
      "preprocessedIA/file5.txt IAgroups/dict5.txt\n",
      "5447\n",
      "preprocessedIA/file6.txt IAgroups/dict6.txt\n",
      "5590\n",
      "preprocessedIA/file7.txt IAgroups/dict7.txt\n",
      "5682\n",
      "preprocessedIA/file8.txt IAgroups/dict8.txt\n",
      "5742\n",
      "preprocessedIA/file9.txt IAgroups/dict9.txt\n",
      "5814\n",
      "preprocessedIA/file10.txt IAgroups/dict10.txt\n",
      "5875\n",
      "preprocessedIA/file11.txt IAgroups/dict11.txt\n",
      "5922\n",
      "preprocessedIA/file12.txt IAgroups/dict12.txt\n",
      "5965\n",
      "preprocessedIA/file13.txt IAgroups/dict13.txt\n",
      "6008\n",
      "preprocessedIA/file14.txt IAgroups/dict14.txt\n",
      "6050\n",
      "preprocessedIA/file15.txt IAgroups/dict15.txt\n",
      "6089\n",
      "preprocessedIA/file16.txt IAgroups/dict16.txt\n",
      "6117\n",
      "preprocessedIA/file17.txt IAgroups/dict17.txt\n",
      "6143\n",
      "preprocessedIA/file18.txt IAgroups/dict18.txt\n",
      "6167\n",
      "preprocessedIA/file19.txt IAgroups/dict19.txt\n",
      "6202\n",
      "preprocessedIA/file20.txt IAgroups/dict20.txt\n",
      "6221\n",
      "preprocessedIA/file21.txt IAgroups/dict21.txt\n",
      "6248\n",
      "preprocessedIA/file22.txt IAgroups/dict22.txt\n",
      "6273\n",
      "preprocessedIA/file23.txt IAgroups/dict23.txt\n",
      "6296\n",
      "preprocessedIA/file24.txt IAgroups/dict24.txt\n",
      "6326\n",
      "preprocessedIA/file25.txt IAgroups/dict25.txt\n",
      "6356\n",
      "preprocessedIA/file26.txt IAgroups/dict26.txt\n",
      "6393\n",
      "preprocessedIA/file27.txt IAgroups/dict27.txt\n",
      "6421\n",
      "preprocessedIA/file28.txt IAgroups/dict28.txt\n",
      "6449\n",
      "preprocessedIA/file29.txt IAgroups/dict29.txt\n",
      "6470\n",
      "preprocessedIA/file30.txt IAgroups/dict30.txt\n",
      "6494\n",
      "preprocessedIA/file31.txt IAgroups/dict31.txt\n",
      "6506\n",
      "preprocessedIA/file32.txt IAgroups/dict32.txt\n"
     ]
    }
   ],
   "source": [
    "fileIn = 'preprocessedIA/file'\n",
    "fileOut = 'IAgroups/dict'\n",
    "word_dict = {}\n",
    "for i in range(0,33):\n",
    "    fileIn1 = fileIn + str(i) + '.txt'\n",
    "    fileOut1 = fileOut + str(i) + '.txt'\n",
    "    word_dict = grupare_abstracte(word_dict,fileIn1,fileOut1 )\n",
    "    print(fileIn1,fileOut1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = open('IAgroups/dict_final.txt','r',encoding='utf-8')\n",
    "word_dict = json.load(r)\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict_sorted = sorted((value,key) for (key,value) in word_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = open('IAgroups/dict_final_sorted.txt','w',encoding='utf-8')\n",
    "w.write(json.dumps(word_dict_sorted))\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6506"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grupare_abstracte(word_dict,abstracts_list,fileIn):\n",
    "    r = open(fileIn,'r',encoding='utf-8')\n",
    "    \n",
    "    for line in r:\n",
    "        crt_paper = json.loads(line)\n",
    "        iabstr = crt_paper[\"indexed_abstract\"]\n",
    "        current_abstr = []\n",
    "        for elem in iabstr:\n",
    "            if word_dict[elem] >= 5 and word_dict[elem] <= 1000:\n",
    "                current_abstr.append(elem)\n",
    "        abstracts_list.append(current_abstr)\n",
    "    r.close()\n",
    "    return abstracts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "fileIn = 'preprocessedIA/file'\n",
    "abstracts_list = []\n",
    "for i in range(0,33):\n",
    "    fileIn1 = fileIn + str(i) + '.txt'\n",
    "    abstracts_list = grupare_abstracte(word_dict,abstracts_list,fileIn1 )\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3257719\n"
     ]
    }
   ],
   "source": [
    "print(len(abstracts_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abide'], [], [], ['fractionate'], [], [], [], [], ['solidify'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['redetermine'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['stylise'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['carve', 'credit'], [], ['equilibrate'], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(abstracts_list[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_abstracts_list = []\n",
    "for elem in abstracts_list:\n",
    "    if elem != []:\n",
    "        new_abstracts_list.append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54711\n"
     ]
    }
   ],
   "source": [
    "print(len(new_abstracts_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abide'], ['fractionate'], ['solidify'], ['redetermine'], ['stylise'], ['carve', 'credit'], ['equilibrate'], ['abridge'], ['incubate'], ['abolish'], ['victimize'], ['succumb'], ['monopolise'], ['proffer'], ['eventuate', 'contest'], ['modernise'], ['elude'], ['creep'], ['disgrace'], ['entice'], ['triple'], ['sedate'], ['allot'], ['major'], ['intrude'], ['fin'], ['irradiate', 'fractionate'], ['occult'], ['stomp'], ['revive'], ['charter'], ['infiltrate'], ['relegate'], ['digitise'], ['itemize'], ['spoil', 'complain'], ['parent'], ['bore'], ['rein'], ['dream'], ['crystallize', 'disengage'], ['prospect'], ['transliterate'], ['miniaturise'], ['await'], ['entice'], ['gentle'], ['redo'], ['oblige'], ['pinch'], ['tout'], ['prod'], ['interpose'], ['assuage'], ['repel'], ['rebind'], ['roleplay'], ['misrepresent'], ['shelter'], ['referee'], ['appoint'], ['puzzle'], ['delocalize'], ['particularize'], ['industrialize'], ['underperform'], ['loaf'], ['peck'], ['relativise'], ['dredge'], ['stumble'], ['discontinue'], ['shun'], ['reply'], ['wipe'], ['blank'], ['striate'], ['attune'], ['sponge'], ['defuse'], ['sprout'], ['abstain'], ['bestow'], ['equalise'], ['apprehend'], ['admire', 'mind'], ['sear', 'loosen', 'reopen'], ['instill'], ['counterbalance'], ['dung'], ['huddle'], ['visa'], ['predate'], ['sanction', 'fin'], ['rub'], ['lexicalise'], ['forestall'], ['spirt'], ['proof'], ['popularise']]\n"
     ]
    }
   ],
   "source": [
    "print(new_abstracts_list[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = open('abstract_5_100.txt','w',encoding='utf-8')\n",
    "w.write(json.dumps(new_abstracts_list))\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
