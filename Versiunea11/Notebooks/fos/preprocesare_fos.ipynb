{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESARE FOS\n",
    "### 1. Creare lista de fos unice\n",
    "### 2. Lower si elimina punctuatia\n",
    "### 3. Elimina stop words\n",
    "### 4. Calculeaza numarul de aparitii\n",
    "### 5. Elimina dupa aparitii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # fisiere\n",
    "from collections import OrderedDict # sortare lista\n",
    "import string # punctiatie si lower\n",
    "from nltk.corpus import stopwords # pentru eliminare stopwords\n",
    "import itertools # pentru sortare\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creare_lista_fos(fileName):\n",
    "    i = 1\n",
    "    r = open(fileName,'r')\n",
    "    fos_list = []\n",
    "    for line in r:\n",
    "        crt_paper = json.loads(line)\n",
    "        current_fos = crt_paper[\"fos\"]\n",
    "        for elem in current_fos:\n",
    "            fos_list.append(elem[\"name\"])\n",
    "        if i % 100000 == 0:\n",
    "            fos_list = list(set(fos_list))\n",
    "            print(i)\n",
    "        i += 1\n",
    "    return fos_list\n",
    "\n",
    "def elimina_punctuatia(list_fos):\n",
    "    new_list_fos = []\n",
    "    \n",
    "    for fos in list_fos:\n",
    "        \n",
    "        word = \"\"\n",
    "        new_current_fos = []\n",
    "        fos = fos.lower()\n",
    "        for ch in fos:\n",
    "            if ch == ' ' or ch =='-':\n",
    "                word = re.sub(r'\\d+', '', word)\n",
    "                new_current_fos.append(word)\n",
    "                word = \"\"\n",
    "            else:\n",
    "                if ch not in string.punctuation:\n",
    "                    word += ch\n",
    "                    \n",
    "        word = re.sub(r'\\d+', '', word)\n",
    "        new_current_fos.append(word)\n",
    "        new_list_fos.append(new_current_fos)\n",
    "        \n",
    "    return new_list_fos\n",
    "\n",
    "def elimina_stopwords(fos_l):\n",
    "    new_fos_l = []\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for fos in fos_l:\n",
    "        new_fos = []\n",
    "        for elem in fos:\n",
    "            if elem not in stop_words and len(elem) > 2:\n",
    "                new_fos.append(elem)\n",
    "        new_fos_l.append(new_fos)\n",
    "    return new_fos_l\n",
    "\n",
    "def calculeaza_nr_aparitii(fos_l):\n",
    "    # Calculez numarul total de cuvinte\n",
    "    words_dict = {}\n",
    "    word_list = []\n",
    "    for fos in fos_l:\n",
    "        for elem in fos:\n",
    "            try:\n",
    "                words_dict[elem] += 1\n",
    "            except:\n",
    "                words_dict[elem] = 1\n",
    "                word_list.append(elem)\n",
    "                \n",
    "    return words_dict, word_list\n",
    "\n",
    "def elimina_cuv_dupa_aparitie(word_dict, frecv_max, frev_min):\n",
    "    new_word_dict = {}\n",
    "    new_word_list = []\n",
    "    \n",
    "    for word in word_dict:\n",
    "        if word_dict[word] < frecv_max and word_dict[word] > frev_min:\n",
    "            new_word_dict[word] = word_dict[word]\n",
    "            new_word_list.append(word)\n",
    "    return new_word_dict, new_word_list\n",
    "\n",
    "def elimina_fos(fos_l,word_list_reduced):\n",
    "    new_fos_l = []\n",
    "    for fos in fos_l:\n",
    "        new_current_fos = []\n",
    "        for elem in fos:\n",
    "            if elem in word_list_reduced:\n",
    "                new_current_fos.append(elem)\n",
    "        new_fos_l.append(new_current_fos)\n",
    "    return new_fos_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functii auxiliare\n",
    "def pretty_print_lista(lista, fisier):\n",
    "    fisier += \"_pretty.txt\"\n",
    "    wFile = open(fisier,'w',encoding='utf-8')\n",
    "    for elem in lista:\n",
    "        wFile.write(elem)\n",
    "        wFile.write('\\n')\n",
    "    wFile.close()\n",
    "    \n",
    "def pretty_print_lista_de_liste(liste, fisier):\n",
    "    fisier += \"_pretty.txt\"\n",
    "    wFile = open(fisier,'w',encoding='utf-8')\n",
    "    for lista in liste:\n",
    "        for elem in lista:\n",
    "            wFile.write(elem)\n",
    "            wFile.write(' ')\n",
    "        wFile.write('\\n')\n",
    "    wFile.close()\n",
    "    \n",
    "def print_lista(lista, fisier):\n",
    "    fisier += \".txt\"\n",
    "    wFile = open(fisier,'w',encoding='utf-8')\n",
    "    wFile.write(json.dumps(lista))\n",
    "    wFile.close()\n",
    "    \n",
    "def print_dict(dictionar, fisier):\n",
    "    fisier += \".txt\"\n",
    "    wFile = open(fisier,'w',encoding='utf-8')\n",
    "    wFile.write(json.dumps(dictionar))\n",
    "    wFile.close()\n",
    "    \n",
    "def readLista(fisier):\n",
    "    fisier += \".txt\"\n",
    "    rFile = open(fisier,'r',encoding='utf-8')\n",
    "    lista = json.load(rFile)\n",
    "    rFile.close()\n",
    "    return lista\n",
    "\n",
    "def sortare_dictionar(word_dict):\n",
    "    word_dict_sorted = {}\n",
    "    for w in sorted(word_dict, key=word_dict.get, reverse=True):\n",
    "        word_dict_sorted[w] = word_dict[w]\n",
    "    return word_dict_sorted\n",
    "\n",
    "def eliminare_duplicate_si_vide(lista):\n",
    "    lista.sort()\n",
    "    sort_lista = list(lista for lista,_ in itertools.groupby(lista))\n",
    "    if sort_lista[0] == []:\n",
    "        return sort_lista[1:]\n",
    "    return sort_lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fos_list = creare_lista_fos('../dblp_data/completeData/dblp_v11_4.json')\n",
    "#fos_list = readLista(\"fos_list\")\n",
    "#print(len(fos_list))\n",
    "#pretty_print_lista(fos_list,\"fos_list\")\n",
    "fl = readLista(\"fos_list\")\n",
    "fl2 = elimina_punctuatia(fl)\n",
    "fl3 = elimina_stopwords(fl2)\n",
    "fos_dict, fos_list_ap = calculeaza_nr_aparitii(fl3)\n",
    "fos_dict_sort = sortare_dictionar(fos_dict)\n",
    "word_dict_reduced_3_10, word_list_reduced_3_10 = elimina_cuv_dupa_aparitie(fos_dict, 1000, 3)\n",
    "fos_list_reduced_3_10 = elimina_fos(fl3,word_dict_reduced_3_10)\n",
    "fos_list_reduced_3_10 = eliminare_duplicate_si_vide(fos_list_reduced_3_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11778\n",
      "57803\n"
     ]
    }
   ],
   "source": [
    "print(len(word_dict_reduced_3_10))\n",
    "print(len(fos_list_reduced_3_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_lista(fos_list_reduced_3_10,\"fos_list_reduced_3_1000\")\n",
    "pretty_print_lista_de_liste(fos_list_reduced_3_10,\"fos_list_reduced_3_1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abacus'], ['abbe'], ['abc'], ['abc', 'flower'], ['abductor'], ['abductor', 'pollicis'], ['abductor', 'pollicis', 'brevis'], ['abductor', 'pollicis', 'longus'], ['aberration'], ['abies'], ['abiotic'], ['able'], ['abort'], ['abortion'], ['abrasive'], ['abrasive', 'blasting'], ['abscissa'], ['absent'], ['absent', 'nipple'], ['absorbed'], ['absorber'], ['absorbing'], ['abundance'], ['acacia'], ['acceptable'], ['accepted'], ['acceptor'], ['accessible'], ['accessory'], ['accidents'], ['accumbens'], ['acetabular'], ['acetabular', 'impingement'], ['acetabulum'], ['acetic'], ['acetic', 'anhydride'], ['acetyltransferase'], ['achromatic'], ['acidemia'], ['acidic'], ['acidosis'], ['acid–base'], ['acid–base', 'titration'], ['ackermann'], ['acl'], ['acquiescence'], ['acquiring'], ['acrylic'], ['acrylic', 'resin'], ['activism'], ['acyl'], ['acyl', 'coa'], ['acyl', 'coenzyme'], ['acyl', 'homoserine'], ['acyl', 'synthetases'], ['ada'], ['adam'], ['adapted'], ['adaptor'], ['adc'], ['adcy'], ['add'], ['addictive'], ['adductor'], ['adductor', 'pollicis'], ['adductor', 'spasmodic', 'dysphonia'], ['adenine', 'dinucleotide'], ['adenoma'], ['adenovirus'], ['adenylyl'], ['adenylyl', 'cyclase'], ['adept'], ['adequacy'], ['adequate'], ['adic'], ['adic', 'hodge'], ['adipose'], ['adipose', 'lipase'], ['adjuvant'], ['adme'], ['admiration'], ['admittance'], ['adoption'], ['adp'], ['advance'], ['adverb'], ['adverbial'], ['advertisement'], ['advice'], ['advisory'], ['aedes'], ['aedes', 'aegypti'], ['aeration'], ['aerodynamic'], ['aeroelasticity'], ['aeromonas'], ['aeronautical'], ['aeruginosa'], ['aesthetic'], ['affected']]\n"
     ]
    }
   ],
   "source": [
    "print(fos_list_reduced_3_10[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
