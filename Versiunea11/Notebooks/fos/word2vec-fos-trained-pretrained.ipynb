{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '../library')\n",
    "\n",
    "from json_file_preprocessing import *\n",
    "from text_preprocessing import *\n",
    "from word2vec import *\n",
    "from helpers import *\n",
    "from clustering import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21364\n"
     ]
    }
   ],
   "source": [
    "r = open('../results/fos/fos_5_100.txt', 'r')\n",
    "fos = json.load(r)\n",
    "r.close()\n",
    "print(len(fos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_glove_100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found 0 words from 35754 words.\n"
     ]
    }
   ],
   "source": [
    "X = apply_model_w2v(fos,model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('coal', 0.7360191345214844), ('mines', 0.7158533334732056), ('copper', 0.6758443713188171), ('industrial', 0.6711450815200806), ('ore', 0.6710669994354248), ('mine', 0.6709348559379578), ('mineral', 0.6708515286445618), ('exploration', 0.6595709323883057), ('minerals', 0.639697790145874), ('logging', 0.6393759250640869)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar('mining'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "xmeans = apply_XMeans(X,1,100)\n",
    "clusters = xmeans.get_clusters()\n",
    "print(len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average silhouette_score is : 0.017756829087519063\n"
     ]
    }
   ],
   "source": [
    "cl = get_labels(clusters)\n",
    "apply_silhouette(X,cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.581179612518452495063554355E+754397\n"
     ]
    }
   ],
   "source": [
    "my_model = train_word2vec(fos, 'fos_s50i40', size=50, window=2, min_count=1, workers=8, iter=100)\n",
    "print(get_perplexity(my_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found 0 words from 35754 words.\n"
     ]
    }
   ],
   "source": [
    "my_X = apply_model_w2v(fos,my_model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ontology', 0.7213804125785828), ('null', 0.7006464004516602), ('migration', 0.6744667291641235), ('analytics', 0.6488808393478394), ('modification', 0.6461018323898315), ('methodology', 0.6411551237106323), ('anthropology', 0.6397405862808228), ('pronoun', 0.6325876712799072), ('breathe', 0.6316117644309998), ('noisy', 0.6288643479347229)]\n"
     ]
    }
   ],
   "source": [
    "print(my_model.wv.most_similar('mining'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "xmeans = apply_XMeans(my_X,1,1000)\n",
    "my_clusters = xmeans.get_clusters()\n",
    "print(len(my_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average silhouette_score is : 0.0144714157531467\n"
     ]
    }
   ],
   "source": [
    "my_cl = get_labels(my_clusters)\n",
    "apply_silhouette(my_X,my_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84. 33. 72. ... 29. 43. 14.]\n"
     ]
    }
   ],
   "source": [
    "print(my_cl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dunn(X, cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "def delta_fast(ck, cl, distances):\n",
    "    values = distances[np.where(ck)][:, np.where(cl)]\n",
    "    values = values[np.nonzero(values)]\n",
    "\n",
    "    return np.min(values)\n",
    "    \n",
    "def big_delta_fast(ci, distances):\n",
    "    values = distances[np.where(ci)][:, np.where(ci)]\n",
    "    #values = values[np.nonzero(values)]\n",
    "            \n",
    "    return np.max(values)\n",
    "    \n",
    "def dunn(points, labels):\n",
    "    \"\"\"  points : np.array   np.array([N, p]) of all points\n",
    "    labels: np.array   np.array([N]) labels of all points\n",
    "    \"\"\"\n",
    "    distances = euclidean_distances(points)\n",
    "    ks = np.sort(np.unique(labels))\n",
    "    \n",
    "    deltas = np.ones([len(ks), len(ks)])*1000000\n",
    "    big_deltas = np.zeros([len(ks), 1])\n",
    "    \n",
    "    l_range = list(range(0, len(ks)))\n",
    "    \n",
    "    for k in l_range:\n",
    "        for l in (l_range[0:k]+l_range[k+1:]):\n",
    "            deltas[k, l] = delta_fast((labels == ks[k]), (labels == ks[l]), distances)\n",
    "        \n",
    "        big_deltas[k] = big_delta_fast((labels == ks[k]), distances)\n",
    "\n",
    "    di = np.min(deltas)/np.max(big_deltas)\n",
    "    return di\n",
    "\n",
    "#k_means = KMeans(n_clusters=i, random_state=0) \n",
    "#    k_means.fit(df) #K-means training \n",
    "#print(dunn(X, cl))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metadata_file()\n",
    "start_tensorboard_p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "\n",
    "def create_metadata_file(metadata, file_path):\n",
    "    w = open(file_path, \"w\") \n",
    "    for elem in metadata:\n",
    "        print(str(elem), file=w)\n",
    "    w.close()\n",
    "\n",
    "def start_tensorboard_p(X, n_components,directory,metadata_list): \n",
    "    PATH = os.getcwd()\n",
    "\n",
    "    LOG_DIR = PATH + directory\n",
    "\n",
    "    metadata = os.path.join(LOG_DIR, metadataFile)\n",
    "\n",
    "    pca = sklearnPCA(n_components=n_components) #2-dimensional PCA\n",
    "    pca_df = pd.DataFrame(pca.fit_transform(X))\n",
    "    df_pca = pca_df.values\n",
    "\n",
    "    tf_data = tf.Variable(df_pca)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver([tf_data])\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.save(sess, os.path.join(LOG_DIR, 'tf_data.ckpt'))\n",
    "        config = projector.ProjectorConfig()\n",
    "\n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = tf_data.name\n",
    "        embedding.metadata_path = metadata\n",
    "\n",
    "    projector.visualize_embeddings(tf.summary.FileWriter(LOG_DIR), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
