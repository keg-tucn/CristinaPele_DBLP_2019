{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.random.normal(\n",
    "    shape,\n",
    "    mean=0.0,\n",
    "    stddev=1.0,\n",
    "    dtype=tf.dtypes.float32,\n",
    "    seed=None,\n",
    "    name=None\n",
    ") -> Outputs random values from a normal distribution\n",
    "\n",
    "tf.nn.softmax(\n",
    "    logits,\n",
    "    axis=None,\n",
    "    name=None,\n",
    "    dim=None\n",
    ")\n",
    "\n",
    "tf.math.reduce_sum(\n",
    "    input_tensor,\n",
    "    axis=None,\n",
    "    keepdims=None,\n",
    "    name=None,\n",
    "    reduction_indices=None,\n",
    "    keep_dims=None\n",
    ") Computes the sum of elements across dimensions of a tensor. \n",
    "Reduces input_tensor along the dimensions given in axis\n",
    "\n",
    "tf.math.reduce_mean(\n",
    "    input_tensor,\n",
    "    axis=None,\n",
    "    keepdims=None,\n",
    "    name=None,\n",
    "    reduction_indices=None,\n",
    "    keep_dims=None\n",
    ") Computes the mean of elements across dimensions of a tensor.\n",
    "Reduces input_tensor along the dimensions given in axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Citest json files si creeaza lista de cuvinte unice din venues. Scrie rezultatul in fisier.\n",
    "### return numarul de cuvinte.\n",
    "###\n",
    "def creeaza_lista_cuvinte_unice():\n",
    "    lista_unica_cuvinte = []\n",
    "    \n",
    "    ## Citeste lista de venues\n",
    "    r = open('../Date/Initiale/lista_venue_completa.txt', 'r')\n",
    "    lista_venues = json.load(r)\n",
    "    r.close()\n",
    "    \n",
    "    ## Creeaza lista de cuvinte unice din venues\n",
    "    for venue in lista_venues:\n",
    "        lista_unica_cuvinte += venue\n",
    "    lista_unica_cuvinte = set(lista_unica_cuvinte)\n",
    "    \n",
    "    w = open('../DATE/lista_unica_cuvinte.txt','w')\n",
    "    w.write(json.dumps(list(lista_unica_cuvinte)))\n",
    "    w.close()\n",
    "\n",
    "    return len(lista_unica_cuvinte)\n",
    "\n",
    "###\n",
    "### Citeste lista de cuvinte unice si creeaza dictionar {cuvant:id}. Scrie rezultatul in fisier.\n",
    "### param file_lista_cuvinte: calea spre fisierul de unde se citeste lista de cuvinte unice.\n",
    "### param out_file: calea spre fisierul unde se va scrie rezultatul\n",
    "###\n",
    "def creeaza_word2int(file_lista_cuvinte, out_file):\n",
    "    # Citeste lista cuvinte\n",
    "    r = open(file_lista_cuvinte,'r')\n",
    "    lista_unica_cuvinte = json.load(r)\n",
    "    r.close()\n",
    "    \n",
    "    word2int = {}\n",
    "    \n",
    "    for i,cuvant in enumerate(lista_unica_cuvinte):\n",
    "        word2int[cuvant] = i\n",
    "        \n",
    "    w = open(out_file,'w')\n",
    "    w.write(json.dumps(word2int))\n",
    "    w.close()\n",
    "    \n",
    "    \n",
    "###\n",
    "### Citeste lista de cuvinte unice si creeaza dictionar {id:cuvant}. Scrie rezultatul in fisier.\n",
    "### param file_lista_cuvinte: calea spre fisierul de unde se citeste lista de cuvinte unice.\n",
    "### param out_file: calea spre fisierul unde se va scrie rezultatul\n",
    "###\n",
    "def creeaza_int2word(file_lista_cuvinte, out_file):\n",
    "    # Citeste lista cuvinte\n",
    "    r = open(file_lista_cuvinte,'r')\n",
    "    lista_unica_cuvinte = json.load(r)\n",
    "    r.close()\n",
    "    \n",
    "    int2word = {}\n",
    "    \n",
    "    for i,cuvant in enumerate(lista_unica_cuvinte):\n",
    "        int2word[i] = cuvant\n",
    "        \n",
    "    w = open(out_file,'w')\n",
    "    w.write(json.dumps(int2word))\n",
    "    w.close()\n",
    "\n",
    "###\n",
    "### Creeaza un vector de tip one hot.\n",
    "### param index: pozitia din vector pe care se va pune 1.\n",
    "### param len_vect: lungimea vectorului.\n",
    "### return hot_vect: vectorul rezultat.\n",
    "###\n",
    "def one_hot(index, len_vect):\n",
    "    hot_vect = np.zeros(len_vect)\n",
    "    hot_vect[index] = 1\n",
    "    return hot_vect\n",
    "\n",
    "###\n",
    "###\n",
    "###\n",
    "def creeaza_lista_vecini(word2int_file, docs_file, DIM_FEREASTRA, DIM_VOCABULAR):\n",
    "    # Citeste word2int\n",
    "    r = open(word2int_file, 'r')\n",
    "    word2int = json.load(r)\n",
    "    r.close()\n",
    "    \n",
    "    # Citeste lista documente\n",
    "    r = open(docs_file, 'r')\n",
    "    lista_doc = json.load(r)\n",
    "    r.close()\n",
    "    \n",
    "    X_train = [] \n",
    "    Y_train = [] \n",
    "    \n",
    "    for doc in lista_doc:\n",
    "        for word_index, word in enumerate(doc):\n",
    "            for vecin in doc[max(word_index - DIM_FEREASTRA, 0) : min(word_index + DIM_FEREASTRA, len(doc)) + 1] : \n",
    "                if vecin != word:\n",
    "                    X_train.append(one_hot(word2int[ word ], DIM_VOCABULAR))\n",
    "                    Y_train.append(one_hot(word2int[ vecin ], DIM_VOCABULAR))\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    Y_train = np.asarray(Y_train)\n",
    "    \n",
    "    return X_train, Y_train\n",
    "\n",
    "def log(word2int_file, vectors, file):\n",
    "    r = open(word2int_file, 'r')\n",
    "    word2int = json.load(r)\n",
    "    r.close()\n",
    "    \n",
    "    dict_ = {}\n",
    "    for elem in word2int:\n",
    "        dict_[elem] = vectors[word2int[elem]].tolist()\n",
    "    w = open (file, 'w')\n",
    "    w.write(json.dumps(dict_))\n",
    "    w.close()       \n",
    "    \n",
    "\n",
    "def train ():\n",
    "    DIM_VOCABULAR = creeaza_lista_cuvinte_unice()\n",
    "    #creeaza_word2int('../DATE/lista_unica_cuvinte.txt','../DATE/word2int.txt')\n",
    "    #creeaza_int2word('../DATE/lista_unica_cuvinte.txt','../DATE/int2word.txt')\n",
    "    \n",
    "    DIM_FEREASTRA = 2\n",
    "    X_train, Y_train = creeaza_lista_vecini('../Date/Initiale/word2int_venues.txt', '../Date/Initiale/lista_venue_completa.txt', DIM_FEREASTRA, DIM_VOCABULAR)\n",
    "    \n",
    "    #### Creare retea\n",
    "    X = tf.placeholder(tf.float32, shape=(None, DIM_VOCABULAR))\n",
    "    Y_eticheta = tf.placeholder(tf.float32, shape=(None, DIM_VOCABULAR))\n",
    "\n",
    "    ## Din layer de input in hidden layer\n",
    "    EMBEDDING_DIM = 50\n",
    "    W1 = tf.Variable(tf.random_normal([DIM_VOCABULAR, EMBEDDING_DIM]))\n",
    "    b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) #bias\n",
    "    layer_hidden = tf.add(tf.matmul(X,W1), b1)\n",
    "\n",
    "    ## Din hidden layer in output layer\n",
    "    W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, DIM_VOCABULAR]))\n",
    "    b2 = tf.Variable(tf.random_normal([DIM_VOCABULAR]))\n",
    "    output_layer = tf.nn.softmax(tf.add( tf.matmul(layer_hidden, W2), b2))\n",
    "    \n",
    "    #### Instantiere model tensorflow\n",
    "    with tf.device('/gpu:0'):\n",
    "        sess = tf.Session()\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Functia de loss -> cross entropy\n",
    "        cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(Y_eticheta * tf.log(tf.clip_by_value(output_layer,1e-10,1.0)), \n",
    "                                                           reduction_indices=[1]))\n",
    "        #is actually a horrible way of computing the cross-entropy. In some samples, certain classes could be excluded with certainty after a while, resulting in y_conv=0 for that sample. That's normally not a problem since you're not interested in those, but in the way cross_entropy is written there, it yields 0*log(0) for that particular sample/class. Hence the NaN.\n",
    "        cross_entropy_loss = tf.reduce_mean(tf.square(output_layer - Y_eticheta))\n",
    "        #cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = Y_eticheta, logits=output_layer))\n",
    "        # compute_gradients + apply_gradients = minimize\n",
    "        learning_rate = 0.01\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
    "        loss_stochastic = []\n",
    "        index_cris = 0\n",
    "        print_nr = 0\n",
    "        while(1):\n",
    "        \n",
    "            \n",
    "            for k in range(DIM_VOCABULAR):\n",
    "                index_cris+=1\n",
    "                #if ( index_cris > 50000 and index_cris % 100 == 0):\n",
    "                #    f_log = 'nf/model' + str(index_cris) + '.txt'#vectors = sess.run(W1 + b1)\n",
    "                #                           \n",
    "                #                     log('../Date/Initiale/word2int_venues.txt', vectors, f_log )\n",
    "                    \n",
    "                rand = random.randint(0,DIM_VOCABULAR)\n",
    "                x_rand = X_train[k].reshape(1,1416) # Transpose to the correct shape\n",
    "                y_rand = Y_train[k].reshape(1,1416)\n",
    "                \n",
    "                sess.run(train_step, feed_dict={X: x_rand, Y_eticheta: y_rand})\n",
    "                temp_loss = sess.run(cross_entropy_loss, feed_dict={X: x_rand, Y_eticheta:y_rand})\n",
    "                \n",
    "                loss_stochastic.append(temp_loss)\n",
    "\n",
    "                if k % 100 == 0:\n",
    "                    print('Loss = ', temp_loss, print_nr, index_cris)\n",
    "                    w = open('nf/lossk22.txt','a')\n",
    "                    w.write(str(temp_loss))\n",
    "                    w.write(' ')\n",
    "                    w.close()\n",
    "                    print_nr+=1\n",
    "                    \n",
    "                if ( index_cris > 100000):\n",
    "                    break\n",
    "            if ( index_cris > 100000):\n",
    "                    break\n",
    "        vectors = sess.run(W1 + b1)       \n",
    "        \n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss =  0.00088747917 0 1\n",
      "Loss =  0.0011346883 1 101\n",
      "Loss =  0.0010267624 2 201\n",
      "Loss =  0.0010676852 3 301\n",
      "Loss =  0.00092934055 4 401\n",
      "Loss =  0.0012536306 5 501\n",
      "Loss =  0.0013659444 6 601\n",
      "Loss =  0.0010954103 7 701\n",
      "Loss =  0.0010076957 8 801\n",
      "Loss =  0.00096114597 9 901\n",
      "Loss =  0.0010612131 10 1001\n",
      "Loss =  0.0014035169 11 1101\n",
      "Loss =  0.0010562738 12 1201\n",
      "Loss =  0.0012654277 13 1301\n",
      "Loss =  0.0012644868 14 1401\n",
      "Loss =  0.0008872826 15 1417\n",
      "Loss =  0.0011340812 16 1517\n",
      "Loss =  0.0010261772 17 1617\n",
      "Loss =  0.0010630402 18 1717\n",
      "Loss =  0.00092909246 19 1817\n",
      "Loss =  0.0012533945 20 1917\n",
      "Loss =  0.0013657719 21 2017\n",
      "Loss =  0.0010954143 22 2117\n",
      "Loss =  0.0010075389 23 2217\n",
      "Loss =  0.0009608691 24 2317\n",
      "Loss =  0.0010615475 25 2417\n",
      "Loss =  0.0014035022 26 2517\n",
      "Loss =  0.0010519549 27 2617\n",
      "Loss =  0.0012640002 28 2717\n",
      "Loss =  0.0012630459 29 2817\n",
      "Loss =  0.0008870903 30 2833\n",
      "Loss =  0.0011334697 31 2933\n",
      "Loss =  0.001025589 32 3033\n",
      "Loss =  0.0010584762 33 3133\n",
      "Loss =  0.00092885195 34 3233\n",
      "Loss =  0.0012531605 35 3333\n",
      "Loss =  0.001365595 36 3433\n",
      "Loss =  0.0010954301 37 3533\n",
      "Loss =  0.0010073856 38 3633\n",
      "Loss =  0.0009605955 39 3733\n",
      "Loss =  0.0010618727 40 3833\n",
      "Loss =  0.0014034862 41 3933\n",
      "Loss =  0.001047816 42 4033\n",
      "Loss =  0.0012625393 43 4133\n",
      "Loss =  0.0012615701 44 4233\n",
      "Loss =  0.00088690076 45 4249\n",
      "Loss =  0.0011328576 46 4349\n",
      "Loss =  0.0010249917 47 4449\n",
      "Loss =  0.0010540052 48 4549\n",
      "Loss =  0.000928617 49 4649\n",
      "Loss =  0.0012529318 50 4749\n",
      "Loss =  0.0013654125 51 4849\n",
      "Loss =  0.0010954571 52 4949\n",
      "Loss =  0.0010072332 53 5049\n",
      "Loss =  0.00096032495 54 5149\n",
      "Loss =  0.0010621997 55 5249\n",
      "Loss =  0.0014034694 56 5349\n",
      "Loss =  0.0010438588 57 5449\n",
      "Loss =  0.0012610593 58 5549\n",
      "Loss =  0.0012600768 59 5649\n",
      "Loss =  0.0008867138 60 5665\n",
      "Loss =  0.0011322486 61 5765\n",
      "Loss =  0.001024392 62 5865\n",
      "Loss =  0.0010496302 63 5965\n",
      "Loss =  0.00092838757 64 6065\n",
      "Loss =  0.0012527098 65 6165\n",
      "Loss =  0.0013652252 66 6265\n",
      "Loss =  0.0010954998 67 6365\n",
      "Loss =  0.0010070803 68 6465\n",
      "Loss =  0.0009600564 69 6565\n",
      "Loss =  0.0010625251 70 6665\n",
      "Loss =  0.001403452 71 6765\n",
      "Loss =  0.0010400878 72 6865\n",
      "Loss =  0.0012595513 73 6965\n",
      "Loss =  0.0012585536 74 7065\n",
      "Loss =  0.00088653126 75 7081\n",
      "Loss =  0.0011316402 76 7181\n",
      "Loss =  0.0010237844 77 7281\n",
      "Loss =  0.0010453573 78 7381\n",
      "Loss =  0.0009281596 79 7481\n",
      "Loss =  0.0012524951 80 7581\n",
      "Loss =  0.0013650298 81 7681\n",
      "Loss =  0.0010955518 82 7781\n",
      "Loss =  0.0010069296 83 7881\n",
      "Loss =  0.0009597923 84 7981\n",
      "Loss =  0.001062848 85 8081\n",
      "Loss =  0.0014034334 86 8181\n",
      "Loss =  0.0010365036 87 8281\n",
      "Loss =  0.0012580084 88 8381\n",
      "Loss =  0.001256996 89 8481\n",
      "Loss =  0.0008863516 90 8497\n",
      "Loss =  0.0011310279 91 8597\n",
      "Loss =  0.0010231716 92 8697\n",
      "Loss =  0.0010411894 93 8797\n",
      "Loss =  0.00092793343 94 8897\n",
      "Loss =  0.0012522866 95 8997\n",
      "Loss =  0.0013648276 96 9097\n",
      "Loss =  0.0010956147 97 9197\n",
      "Loss =  0.0010067808 98 9297\n",
      "Loss =  0.00095953204 99 9397\n",
      "Loss =  0.0010631742 100 9497\n",
      "Loss =  0.0014034132 101 9597\n",
      "Loss =  0.0010331046 102 9697\n",
      "Loss =  0.001256433 103 9797\n",
      "Loss =  0.0012554071 104 9897\n",
      "Loss =  0.00088617567 105 9913\n",
      "Loss =  0.0011304095 106 10013\n",
      "Loss =  0.0010225474 107 10113\n",
      "Loss =  0.0010371316 108 10213\n",
      "Loss =  0.0009277126 109 10313\n",
      "Loss =  0.0012520866 110 10413\n",
      "Loss =  0.0013646187 111 10513\n",
      "Loss =  0.0010956979 112 10613\n",
      "Loss =  0.0010066312 113 10713\n",
      "Loss =  0.00095927436 114 10813\n",
      "Loss =  0.0010634941 115 10913\n",
      "Loss =  0.0014033924 116 11013\n",
      "Loss =  0.0010298933 117 11113\n",
      "Loss =  0.0012548314 118 11213\n",
      "Loss =  0.0012537913 119 11313\n",
      "Loss =  0.00088600384 120 11329\n",
      "Loss =  0.0011297916 121 11429\n",
      "Loss =  0.0010219202 122 11529\n",
      "Loss =  0.0010331909 123 11629\n",
      "Loss =  0.00092749373 124 11729\n",
      "Loss =  0.0012518928 125 11829\n",
      "Loss =  0.0013644048 126 11929\n",
      "Loss =  0.0010957868 127 12029\n",
      "Loss =  0.0010064866 128 12129\n",
      "Loss =  0.00095902215 129 12229\n",
      "Loss =  0.0010638108 130 12329\n",
      "Loss =  0.0014033711 131 12429\n",
      "Loss =  0.0010268586 132 12529\n",
      "Loss =  0.0012532043 133 12629\n",
      "Loss =  0.0012521483 134 12729\n",
      "Loss =  0.0008858374 135 12745\n",
      "Loss =  0.0011291733 136 12845\n",
      "Loss =  0.0010212898 137 12945\n",
      "Loss =  0.0010293602 138 13045\n",
      "Loss =  0.00092727813 139 13145\n",
      "Loss =  0.0012517071 140 13245\n",
      "Loss =  0.0013641844 141 13345\n",
      "Loss =  0.001095891 142 13445\n",
      "Loss =  0.0010063418 143 13545\n",
      "Loss =  0.00095877127 144 13645\n",
      "Loss =  0.001064129 145 13745\n",
      "Loss =  0.0014033486 146 13845\n",
      "Loss =  0.0010240009 147 13945\n",
      "Loss =  0.0012515492 148 14045\n",
      "Loss =  0.0012504765 149 14145\n",
      "Loss =  0.00088567316 150 14161\n",
      "Loss =  0.0011285509 151 14261\n",
      "Loss =  0.0010206527 152 14361\n",
      "Loss =  0.0010256429 153 14461\n",
      "Loss =  0.0009270676 154 14561\n",
      "Loss =  0.0012515266 155 14661\n",
      "Loss =  0.0013639559 156 14761\n",
      "Loss =  0.0010960104 157 14861\n",
      "Loss =  0.0010061924 158 14961\n",
      "Loss =  0.0009585223 159 15061\n",
      "Loss =  0.0010644402 160 15161\n",
      "Loss =  0.0014033247 161 15261\n",
      "Loss =  0.0010213171 162 15361\n",
      "Loss =  0.0012498632 163 15461\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-8323bd7a7bfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-05e8dbff38bc>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[0my_rand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1416\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_rand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_eticheta\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_rand\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m                 \u001b[0mtemp_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_rand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_eticheta\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_rand\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user04\\anaconda3\\envs\\dblp2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user04\\anaconda3\\envs\\dblp2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user04\\anaconda3\\envs\\dblp2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user04\\anaconda3\\envs\\dblp2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user04\\anaconda3\\envs\\dblp2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user04\\anaconda3\\envs\\dblp2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectors = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss is :  5.676243 18087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = open('nf/model_001_2.txt', )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
