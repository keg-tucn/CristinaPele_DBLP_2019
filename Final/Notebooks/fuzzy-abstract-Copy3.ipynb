{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('../library')\n",
    "from clustering import *\n",
    "from helpers import *\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(X):\n",
    "    X_list = []\n",
    "    for elem in X:\n",
    "        X_list.append(X[elem])\n",
    "    return X_list\n",
    "\n",
    "def get_keys(X):\n",
    "    X_list = []\n",
    "    for elem in X:\n",
    "        X_list.append(elem)\n",
    "    return X_list\n",
    "\n",
    "###\n",
    "### Transforma un obiect fcm [[elems_clasa_0],..,[elem_clasa_n]] in lista de labels [class_elem_0, .., class_elem_n].\n",
    "### param labels: obiect de tip fcm.\n",
    "### return lista.\n",
    "###\n",
    "def create_class_labels(labels,venues, size):\n",
    "    l = np.zeros(size, dtype=int)\n",
    "    i = 0\n",
    "    for class_ in labels:\n",
    "        for elem in class_:\n",
    "            l[elem] = i\n",
    "        i += 1\n",
    "    dict_ = {}\n",
    "    for i,elem in enumerate(venues):\n",
    "        dict_[elem] = int(l[i])\n",
    "    return dict_\n",
    "\n",
    "def run_fuzzy(path, cls, camp = 'd2w', print_ = False):\n",
    "    X = {}\n",
    "    r = open(path, 'r')\n",
    "    for line in r:\n",
    "        art_crt = json.loads(line)\n",
    "        X[art_crt['id']] = art_crt[camp]\n",
    "    r.close()\n",
    "    \n",
    "    X_values = get_values(X)\n",
    "    X_keys = get_keys(X)\n",
    "    \n",
    "    if print_ == True:\n",
    "        print('size vector ',len(X), len(X_values))\n",
    "    fcm = apply_fuzzy(X_values,cls)\n",
    "    labels = fcm.get_membership()\n",
    "    \n",
    "    i = 0\n",
    "    X_dict = {}\n",
    "    for elem in labels:\n",
    "        X_dict[X_keys[i]] = elem \n",
    "        i+=1\n",
    "    return X_dict, cls\n",
    "    \n",
    "\n",
    "###\n",
    "### Transforma lista de weights a clusterelor in dictionar de forma {cluster:weight}, daca weight este mai \n",
    "### mare decat mijlocul de valori din weights initial.\n",
    "### param weights: lista de weights initiala.\n",
    "### return dict_ sau -1 daca nu este nici o valoare din lista  mai maire de valoarea de middle.\n",
    "###\n",
    "def get_class_weights(weights):\n",
    "    dict_ = {}\n",
    "    min_ = min(weights)\n",
    "    max_ = max(weights)\n",
    "    middle = (max_ + min_)/2\n",
    "    for i in range(0,len(weights)):\n",
    "        if weights[i] >= middle:\n",
    "            dict_[i] = weights[i]\n",
    "            \n",
    "    if len(dict_) == 0:\n",
    "        return -1\n",
    "    return dict_\n",
    "\n",
    "\n",
    "    \n",
    "def print_results(mem, path_in, path_out, nrFiles, space= False):\n",
    "    excluded = 0\n",
    "    for i in range(0, nrFiles):\n",
    "        r = open(path_in+str(i)+'.txt', 'r')\n",
    "        w = open(path_out+str(i)+'.txt', 'w')\n",
    "        \n",
    "        for line in r:\n",
    "            art_crt = json.loads(line)\n",
    "            new_art = {}\n",
    "            new_art['id'] = art_crt['id']\n",
    "            new_art['venue'] = art_crt['venue']\n",
    "            new_art['abstract'] = art_crt['abstract']\n",
    "            new_art['fos'] = art_crt['fos']\n",
    "            try:\n",
    "                new_art['class_weights'] = get_class_weights(mem[new_art['id']])\n",
    "            except KeyError:\n",
    "                excluded += 1\n",
    "            \n",
    "            w.write(json.dumps(new_art))\n",
    "            w.write('\\n')\n",
    "            \n",
    "        w.close()\n",
    "        r.close()\n",
    "    return excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10\n"
     ]
    }
   ],
   "source": [
    "mem, cls = run_fuzzy('../Date/doc2vec/Abstract/Subset/file0.txt', 10)\n",
    "print(len(mem), cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001c58d3-26ad-46b3-ab3a-c1e557d16821 [0.49999815964999705, 0.5000018403500031]\n"
     ]
    }
   ],
   "source": [
    "for elem in mem_max:\n",
    "    print (elem, mem_max[elem])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_results(mem, '../Date/Initiale/Subset/file', '../Rezultate/Fuzzy/Abstract/doc2vec/10clus_file', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 16\n"
     ]
    }
   ],
   "source": [
    "mem,cls = run_fuzzy('../Date/word2vec-pretrained/Abstract/Subset/avg/file0.txt', 16,'abstract')\n",
    "print(len(mem), cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "excluded = print_results(mem, '../Date/Initiale/Subset/file', '../Rezultate/Fuzzy/Abstract/word2vec-pretrained/avg/16clus_file', 1, True)\n",
    "print(excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem,cls = run_fuzzy('../Date/word2vec-pretrained/Abstract/Subset/sum/file0.txt', 18, 'abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "excluded = print_results(mem, '../Date/Initiale/Complet/file', '../Rezultate/Fuzzy/Abstract/word2vec-pretrained/sum/18clus_file', 1, True)\n",
    "print(excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 24\n"
     ]
    }
   ],
   "source": [
    "mem,cls = run_fuzzy('../Date/word2vec-gensim/Abstract/Subset/avg/file0.txt', 24, 'abstract')\n",
    "print(len(mem), cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "excluded = print_results(mem, '../Date/Initiale/Complet/file', '../Rezultate/Fuzzy/Abstract/word2vec-gensim/avg/24clus_file', 1, True)\n",
    "print(excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 20\n"
     ]
    }
   ],
   "source": [
    "mem,cls = run_fuzzy('../Date/word2vec-gensim/Abstract/Subset/sum/file0.txt', 20, 'abstract')\n",
    "print(len(mem), cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "excluded = print_results(mem, '../Date/Initiale/Complet/file', '../Rezultate/Fuzzy/Abstract/word2vec-gensim/sum/20clus_file', 1, True)\n",
    "print(excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Verifica daca o cheie se gaseste intr-un dictionar si returneaza valoarea din dictionar.\n",
    "### param dict_cluster: dictionarul.\n",
    "### param nr_cluster: cheia\n",
    "### return True, valoare daca gaseste sau False, -1 daca nu gaseste\n",
    "###\n",
    "def is_in_cluster(dict_clusters, nr_cluster):\n",
    "    try :\n",
    "        return True, dict_clusters[nr_cluster]\n",
    "    except KeyError:\n",
    "        return False, -1\n",
    "\n",
    "#---\n",
    "###\n",
    "### Parcurge fisierele cu weight_cluster si creeaza un dictionar de forma {venue:weight} pentru un cluster dat.\n",
    "### param cluster: numarul clusterului pentru care se face cautarea (! In format string)\n",
    "### param path: calea spre fisiere\n",
    "### param nrFiles: numarul de fisiere\n",
    "### return cluster_dict: dictionarul cuu venues si weight asociat pentru clusterul cerut\n",
    "###\n",
    "def extragere_puncte_cluster(cluster, path, nrFiles):\n",
    "    cluster_dict = {}\n",
    "    for i in range(0, nrFiles):\n",
    "        file = path+str(i)+'.txt'\n",
    "        r = open(file,'r')\n",
    "        for line in r:\n",
    "            \n",
    "            art_crt = json.loads(line)\n",
    "            try:\n",
    "                includes, weight = is_in_cluster(art_crt['class_weights'],cluster) \n",
    "            except KeyError:\n",
    "                continue\n",
    "            if (includes == True):\n",
    "                id_ = art_crt['id']\n",
    "                try :\n",
    "                    cluster_dict[id_] += weight \n",
    "                except KeyError:\n",
    "                    cluster_dict[id_] = weight   \n",
    "        r.close()\n",
    "    return cluster_dict\n",
    "\n",
    "\n",
    "#--\n",
    "def calcul_inter_cluster(model, cluster_dict): \n",
    "    # am un dict {venue: weight}\n",
    "    # am modelul care face corespondenta venue -> embedding\n",
    "    sum_total = 0\n",
    "    print(len(cluster_dict))\n",
    "    for elem_crt in cluster_dict:\n",
    "        for elem in cluster_dict:\n",
    "            if elem_crt == elem:\n",
    "                continue\n",
    "            sum_ = calculeaza_distanta(model[elem], model[elem_crt])\n",
    "            sum_ *= cluster_dict[elem]\n",
    "        sum_total += sum_\n",
    "        \n",
    "    return sum_total / len(cluster_dict)\n",
    "\n",
    "#--\n",
    "## cluster_dict : {venue:weight}\n",
    "def calc_centro(model, cluster_dict, dim): \n",
    "    sum_total = 0\n",
    "    \n",
    "    centroid = []\n",
    "    for i in range(0, dim):\n",
    "        centroid.append(0)\n",
    "    \n",
    "    for elem in cluster_dict:\n",
    "        elem_m = model[elem]\n",
    "        for i in range(0, dim):\n",
    "            centroid[i] += elem_m[i]\n",
    "            \n",
    "    for i in range(0, dim):\n",
    "        centroid[i] /= dim\n",
    "    return centroid\n",
    "\n",
    "# -- \n",
    "def calcul_intra_cluster(centroid_list):\n",
    "    dist_total = 0\n",
    "    \n",
    "    for centroid_crt in centroid_list:\n",
    "        dist = 0\n",
    "        for centroid in centroid_list:\n",
    "            if centroid == centroid_list:\n",
    "                continue\n",
    "            dist += calculeaza_distanta(centroid_crt, centroid)\n",
    "        dist_total += (dist/(len(centroid_list)-1))\n",
    "    print('CENTRO', dist)\n",
    "    return dist / len(centroid_list)\n",
    "#---\n",
    "def process(nrClustere, cls, path, nrFiles, size_model):\n",
    "    total = 0\n",
    "    centroid_list = []\n",
    "    for i in range(0,nrClustere):\n",
    "        cluster_dict = extragere_puncte_cluster(cls[i],path, nrFiles)\n",
    "        total += calcul_inter_cluster(X_dict, cluster_dict)\n",
    "        centroid_list.append(calc_centro(X_dict, cluster_dict, size_model))\n",
    "        \n",
    "    intra = calcul_intra_cluster(centroid_list)\n",
    "    total /= nrClustere    \n",
    "    return total, intra    \n",
    "\n",
    "## citeste modelul si elimina spatiul final din chei\n",
    "def read_model(path):\n",
    "    r = open(path, 'r')\n",
    "    X_dict = json.load(r)\n",
    "    r.close()\n",
    "    new_x_dict = {}\n",
    "    for elem in X_dict:\n",
    "        new_elem = elem[0:len(elem)-1]\n",
    "        new_x_dict[new_elem] = X_dict[elem]\n",
    "    return new_x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dict = {}\n",
    "r = open('../Date/doc2vec/Abstract/Subset/file0.txt', 'r')\n",
    "for line in r:\n",
    "    art_crt = json.loads(line)\n",
    "    X_dict[art_crt['id']] = art_crt['d2w']\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5075\n",
      "5126\n",
      "4808\n",
      "5127\n",
      "4842\n",
      "4894\n",
      "4870\n",
      "4768\n",
      "5086\n",
      "5209\n",
      "CENTRO 119.52463538168058\n",
      "0.9638868020669757 11.952463538168058\n"
     ]
    }
   ],
   "source": [
    "cls = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "total, intra = process(10,cls,'../Rezultate/Fuzzy/Abstract/doc2vec/10clus_file', 1, 300)\n",
    "#                                  Rezultate\\Fuzzy\\Abstract\\doc2vec\n",
    "print(total, intra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec pretrained avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4947\n"
     ]
    }
   ],
   "source": [
    "cls = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15']\n",
    "\n",
    "total, intra = process(16,cls,'../Rezultate/Fuzzy/Abstract/word2vec-pretrained/avg/16clus_file', 1, 50)\n",
    "print(total, intra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec pretrained sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5185\n",
      "4814\n",
      "5240\n",
      "5223\n",
      "4812\n",
      "4886\n",
      "4824\n",
      "5198\n",
      "5204\n",
      "4823\n",
      "4852\n",
      "4825\n",
      "4814\n",
      "5204\n",
      "4889\n",
      "5184\n",
      "5184\n",
      "5186\n",
      "CENTRO 582.1423763954053\n",
      "0.4121444496119219 32.34124313307807\n"
     ]
    }
   ],
   "source": [
    "cls = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17']\n",
    "\n",
    "total, intra = process(18,cls,'../Rezultate/Fuzzy/Abstract/word2vec-pretrained/sum/18clus_file', 1, 50)\n",
    "print(total, intra) # THIS!!!!!!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec gensim avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23']\n",
    "\n",
    "total, intra = process(24,cls,'../Rezultate/Fuzzy/Abstract/word2vec-gensim/avg/24clus_file', 1, 50)\n",
    "print(total, intra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec gensim sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_dict = read_model('../Date/word2vec-gensim/Venue/w2v_gensim_sum_50_c.txt')\n",
    "cls = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19']\n",
    "\n",
    "total, intra = process(20,cls,'../Rezultate/Fuzzy/Abstract/word2vec-gensim/sum/20clus_file', 1, 50)\n",
    "print(total, intra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
